{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install wheels for Basemap\n",
    "- install Proj: https://proj.org/install.html#install\n",
    "- go to above link >> find Windows: click OSGeo4W >> download 64bit >> following above link's Window section to isntall PROJ\n",
    "- install basemap wheel and pyproj wheel from link: https://www.lfd.uci.edu/~gohlke/pythonlibs/\n",
    "- find: Basemap: a matplotlib toolkit for plotting 2D data on maps based on GEOS. \n",
    "- find: Pyproj: an interface to the PROJ library for cartographic transformations.\n",
    "- #### Important: pip install numpy --upgrade ###\n",
    "\n",
    "### Install wheels for geopandas \n",
    "Installing geopandas and its dependencies manually\n",
    "refer to: https://stackoverflow.com/questions/34427788/how-to-successfully-install-pyproj-and-geopandas\n",
    "\n",
    "Installing geopandas and its dependencies manually\n",
    "\n",
    "1. First and most important: do not try to directly pip install or conda install any of the dependencies – if you do, they will fail in some way later, often silently or obscurely, making troubleshooting difficult. If any are already installed, uninstall them now.\n",
    "\n",
    "2. Download the wheels for GDAL, Fiona, pyproj, rtree, and shapely from Gohlke. Make sure you choose the wheel files that match your architecture (64-bit) and Python version (2.7 or 3.5). If Gohlke mentions any prerequisites in his descriptions of those 5 packages, install the prerequisites now (there might be a C++ redistributable or something similar listed there)\n",
    "\n",
    "3. If OSGeo4W, GDAL, Fiona, pyproj, rtree, or shapely is already installed, uninstall it now. The GDAL wheel contains a complete GDAL installation – don’t use it alongside OSGeo4W or other distributions.\n",
    "\n",
    "4. Open a command prompt and change directories to the folder where you downloaded these 5 wheels.\n",
    "\n",
    "5. pip install the GDAL wheel file you downloaded. Your actual command will be something like: pip install\n",
    "GDAL-1.11.2-cp27-none-win_amd64.whl\n",
    "\n",
    "6. Add the new GDAL path to the windows PATH environment variable, something like C:\\Anaconda\\Lib\\site-packages\\osgeo\n",
    "pip install your Fiona wheel file, then your pyproj wheel file, then rtree, and then shapely.\n",
    "\n",
    "7. Now that GDAL and geopandas’s dependencies are all installed, you can just pip install geopandas from the command prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MilkRun Initial Routing Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\u279014\\\\Documents\\\\H_Drive\\\\7.AA Models\\\\12.Logistic_Optimization\\\\data'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import general packages:\n",
    "from openpyxl import load_workbook\n",
    "import win32com.client\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Grouper\n",
    "from pandas import Timestamp\n",
    "import os\n",
    "import io\n",
    "import datetime as dt\n",
    "import time \n",
    "import feather\n",
    "import itertools\n",
    "from math import sqrt\n",
    "import csv\n",
    "import dask.dataframe as dd\n",
    "from datetime import datetime\n",
    "import timestring\n",
    "from IPython.core.display import display, HTML\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "# import modeling packages\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing, datasets\n",
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "from scipy.spatial.distance import cdist,pdist\n",
    "from scipy import stats\n",
    "from scipy.sparse import *\n",
    "\n",
    "# import visualization packages:\n",
    "from matplotlib import pyplot as plt\n",
    "# from mpl_toolkits.basemap import Basemap\n",
    "import seaborn as sns\n",
    "# import ggplot\n",
    "%matplotlib inline\n",
    "\n",
    "# checking path and dir\n",
    "os.chdir('C:\\\\Users\\\\u279014\\\\Documents\\\\H_Drive\\\\7.AA Models\\\\12.Logistic_Optimization\\\\data')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from ortools.constraint_solver import routing_enums_pb2\n",
    "from ortools.constraint_solver import pywrapcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_on_sphere_numpy(coordinate_df):\n",
    "    \"\"\"\n",
    "    Compute a distance matrix of the coordinates using a spherical metric.\n",
    "    :param coordinate_array: numpy.ndarray with shape (n,2); latitude is in 1st col, longitude in 2nd.\n",
    "    :returns distance_mat: numpy.ndarray with shape (n, n) containing distance in km between coords.\n",
    "    \"\"\"\n",
    "    # Radius of the earth in km (GRS 80-Ellipsoid)\n",
    "    EARTH_RADIUS = 6371.007176\n",
    "    km2mile_ratio = 0.62137\n",
    "\n",
    "    # Unpacking coordinates\n",
    "    latitudes = coordinate_df.loc[:,'latitude']\n",
    "    longitudes = coordinate_df.loc[:,'longitude']\n",
    "\n",
    "    # Convert latitude and longitude to spherical coordinates in radians.\n",
    "    degrees_to_radians = np.pi/180.0\n",
    "    phi_values = (90.0 - latitudes)*degrees_to_radians\n",
    "    theta_values = longitudes*degrees_to_radians\n",
    "\n",
    "    # Expand phi_values and theta_values into grids\n",
    "    theta_1, theta_2 = np.meshgrid(theta_values, theta_values)\n",
    "    theta_diff_mat = theta_1 - theta_2\n",
    "\n",
    "    phi_1, phi_2 = np.meshgrid(phi_values, phi_values)\n",
    "\n",
    "    # Compute spherical distance from spherical coordinates\n",
    "    angle = (np.sin(phi_1) * np.sin(phi_2) * np.cos(theta_diff_mat) + \n",
    "           np.cos(phi_1) * np.cos(phi_2))\n",
    "    arc = np.arccos(angle)\n",
    "\n",
    "    # Multiply by earth's radius to obtain distance in km\n",
    "    return np.nan_to_num(arc * EARTH_RADIUS * km2mile_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def riding_distance(riding_distance_matrix, geo):\n",
    "    \"\"\"\n",
    "    Compute a distance matrix of the coordinates using a spherical metric.\n",
    "    :param  \n",
    "        coordinate_df: numpy.ndarray with shape (n,n); riding_distance_matri: dataframe, col & index type: str \n",
    "        geo_zipcode: Data.Series, element type: str\n",
    "    :returns distance_mat: numpy.ndarray with shape (n, n) containing distance in km between coords.\n",
    "    \"\"\"\n",
    "    d_matrix = []\n",
    "    zipcodes = geo['zip_code'].apply(lambda x: str(x))\n",
    "    for i in zipcodes:\n",
    "        d_row = []\n",
    "        for j in zipcodes:\n",
    "            d_row.append(riding_distance_matrix.loc[i,j])\n",
    "        d_matrix.append(d_row)\n",
    "    return np.asarray(d_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_riding_distance_matrix(path,file):\n",
    "    riding_distance_matrix = pd.read_excel(os.path.join(path,file)).set_index('zipcode')\n",
    "    riding_distance_matrix.columns = riding_distance_matrix.columns.astype('str')\n",
    "    riding_distance_matrix.index = riding_distance_matrix.index.astype('str')\n",
    "    return riding_distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_model(distance_matrix=0, \n",
    "                      ship_weight_list = 0, \n",
    "                      each_vehicle_capacity = 45000, \n",
    "                      num_vehicles = 30,\n",
    "                      nrLocations = 9):\n",
    "    \"\"\"Stores the data for the problem.\"\"\"\n",
    "    data = {}\n",
    "    data['distance_matrix']=distance_matrix\n",
    "    data['demands'] = ship_weight_list\n",
    "    data['vehicle_capacities'] = [each_vehicle_capacity]*num_vehicles\n",
    "    data['num_vehicles'] = num_vehicles\n",
    "    data['depot']=0\n",
    "    data['nrLocations'] = nrLocations\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" optimize algorithm for accurate route \"\"\"\n",
    "def print_solution_3(data, manager, routing, assignment):\n",
    "    \"\"\"Prints assignment on console.\"\"\"\n",
    "    total_distance = 0\n",
    "    total_load = 0\n",
    "    \n",
    "    vehicle_routes = dict() # for list out the same truck pick zipcodes\n",
    "\n",
    "    for vehicle_id in range(data['num_vehicles']):\n",
    "        index = routing.Start(vehicle_id)\n",
    "        plan_output = 'Route for vehicle {}:\\n'.format(vehicle_id)\n",
    "        plan_output_backward = 'Route for vehicle {}:\\n'.format(vehicle_id) # if backward is shorter path\n",
    "        route_distance = 0\n",
    "        route_load = 0\n",
    "        edge_distance = []\n",
    "        while not routing.IsEnd(index):\n",
    "            node_index = manager.IndexToNode(index)\n",
    "            route_load += data['demands'][node_index]\n",
    "            plan_output += ' {0} Load({1}) -> '.format(node_index, route_load)\n",
    "            plan_output_backward += ' {0} Load({1}) <- '.format(node_index, route_load) # if backward is shorter path\n",
    "            \n",
    "            previous_index = index            \n",
    "            index = assignment.Value(routing.NextVar(index))\n",
    "            \n",
    "            if vehicle_id in vehicle_routes:\n",
    "                vehicle_routes[vehicle_id].append(node_index)   # adding zipcodes to same truck\n",
    "            else:\n",
    "                vehicle_routes[vehicle_id] = [node_index]\n",
    "            \n",
    "            route_distance += routing.GetArcCostForVehicle(previous_index, index, vehicle_id)\n",
    "            edge_distance.append(routing.GetArcCostForVehicle(previous_index, index, vehicle_id))\n",
    "        \n",
    "        # adding destination to entire route\n",
    "\n",
    "        \"\"\" this situation is Fudging Headacheeeeeeee\"\"\"\n",
    "        # distance from greenville to first supplier is larger than last supplier to greenville, \n",
    "        # truck starts from first supplier, remove first span of driving from VRP\n",
    "        if edge_distance[0] >= edge_distance[-1]:\n",
    "            vehicle_routes[vehicle_id].append(0)\n",
    "            vehicle_routes[vehicle_id].pop(0)\n",
    "            route_distance = route_distance - edge_distance[0]\n",
    "            plan_output += ' {0} Load({1})\\n'.format(manager.IndexToNode(index),route_load)\n",
    "            plan_output += 'Distance of the route: {} miles\\n'.format(route_distance)\n",
    "            plan_output += 'Load of the route: {}\\n'.format(route_load)\n",
    "\n",
    "#             print(plan_output.replace('0 Load(0) ->  ',''))\n",
    "            total_distance += route_distance\n",
    "            total_load += route_load\n",
    "        \n",
    "        # truck starts form last supplier,remove last span of driving from VRP\n",
    "        else:\n",
    "            route_distance = route_distance - edge_distance[-1]\n",
    "            vehicle_routes[vehicle_id] = vehicle_routes[vehicle_id][::-1]\n",
    "            plan_output_backward += ' {0} Load({1})\\n'.format(manager.IndexToNode(index),route_load)\n",
    "            plan_output_backward += 'Distance of the route: {} miles\\n'.format(route_distance)\n",
    "            plan_output_backward += 'Load of the route: {}\\n'.format(route_load)\n",
    "#             print(plan_output_backward)\n",
    "            total_distance += route_distance\n",
    "            total_load += route_load\n",
    "#     print('Total distance of all routes: {} miles'.format(total_distance))\n",
    "#     print('Total load of all routes: {}'.format(total_load))\n",
    "    return vehicle_routes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Modeling Start >>>>>>\n",
    "## 1. Data_prep\n",
    "### 1.1 load saved feather supplier-cluster dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dictionary for osk_hub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cass_zip_cluster = pd.read_csv('cass_zip_cluster.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_copy = cass_zip_cluster.copy() # make a copy of original dataset\n",
    "cluster_copy = cluster_copy[cluster_copy.label != -1] # drop label(cluser) = -1, which do not belong to any group\n",
    "cluster_copy['shipping_date'] = '10-01-2019'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_copy['zip_code'] = cluster_copy.zip_code.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster_copy = cluster_copy[cluster_copy.ship_weight_freq_median < 45000].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 choose supplier-cluster to run milkrun Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_columns = ['shipper_zip', 'shipper_name', 'freq', 'ship_weight',\n",
    "       'ship_weight_annum', 'shipment_count_annum', 'billed_amount_annum',\n",
    "       'zip_code', 'longitude', 'latitude', 'state_abbreviation', 'label',\n",
    "       'shipping_date']\n",
    "\n",
    "\n",
    "freqs = ['weekly', 'bi_weekly', 'monthly']\n",
    "\n",
    "cluster_copy.columns = replace_columns\n",
    "\n",
    "num_v = 30\n",
    "num_stops = 7\n",
    "v_capacity = 45000\n",
    "n_route_location = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vrp_master(distance_matrix):\n",
    "\n",
    "    # Initiate data problem\n",
    "    _data = create_data_model(distance_matrix=distance_matrix,\n",
    "                              ship_weight_list=ship_weight_list_toy,\n",
    "                              each_vehicle_capacity=v_capacity,\n",
    "                              num_vehicles=num_v,\n",
    "                              nrLocations=n_route_location)\n",
    "\n",
    "    # Create routing index manager\n",
    "    manager = pywrapcp.RoutingIndexManager(len(_data['distance_matrix']),_data['num_vehicles'],_data['depot'])\n",
    "\n",
    "    # Create Routing Model\n",
    "    routing = pywrapcp.RoutingModel(manager)\n",
    "        # Register transit callback\n",
    "    def distance_callback(from_index, to_index):\n",
    "        from_node = manager.IndexToNode(from_index)\n",
    "        to_node = manager.IndexToNode(to_index)\n",
    "        return _data['distance_matrix'][from_node][to_node]\n",
    "    \n",
    "    transit_callback_index = routing.RegisterTransitCallback(distance_callback)\n",
    "    \n",
    "    # Define cost of each arch\n",
    "    routing.SetArcCostEvaluatorOfAllVehicles(transit_callback_index)\n",
    "    \n",
    "    # Add count_stops constraint\n",
    "    count_stop_callback = routing.RegisterUnaryTransitCallback(lambda index: 1)\n",
    "    dimension_name = 'Counter'\n",
    "    routing.AddDimension(count_stop_callback, 0, v_capacity, True, 'Counter')\n",
    "    \n",
    "    counter_dimension = routing.GetDimensionOrDie(dimension_name)\n",
    "    \n",
    "    # add sovler to count stop numbers  \n",
    "    for vehicle_id in range(num_v):\n",
    "        index = routing.End(vehicle_id)\n",
    "        solver = routing.solver()\n",
    "        solver.Add(counter_dimension.CumulVar(index) <= num_stops)\n",
    "        \n",
    "    # Add Capacity constraint\n",
    "    def demand_callback(from_index):\n",
    "        from_code = manager.IndexToNode(from_index)\n",
    "        return _data['demands'][from_code]\n",
    "    \n",
    "    demand_callback_index = routing.RegisterUnaryTransitCallback(demand_callback)\n",
    "\n",
    "    routing.AddDimensionWithVehicleCapacity(demand_callback_index, \n",
    "                                            0,                            # null capacity slack\n",
    "                                            _data['vehicle_capacities'],  # vehicle maximum capacities\n",
    "                                            True,                         # start cumul to zero\n",
    "                                            'Capacity')\n",
    "    \n",
    "    # Adding penalty for loading weight exceeds truck capacity\n",
    "    penalty = 10000\n",
    "    for node in range(1, len(_data['distance_matrix'])):\n",
    "        routing.AddDisjunction([manager.NodeToIndex(node)], penalty)\n",
    "        \n",
    "    # Setting first solution heuristic.\n",
    "    search_parameters = pywrapcp.DefaultRoutingSearchParameters()\n",
    "    search_parameters.first_solution_strategy = (routing_enums_pb2.FirstSolutionStrategy.PATH_CHEAPEST_ARC)\n",
    "    \n",
    "    # Solve the problem.\n",
    "    assignment = routing.SolveWithParameters(search_parameters)\n",
    "    \n",
    "    if assignment:\n",
    "        route_dictionary = print_solution_3(_data,manager,routing,assignment)\n",
    "        \n",
    "    return route_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_schedule(route_dictionary, b=''):\n",
    "    \"\"\" generat truck:pick_node map in dataFrame \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    for k in route_dictionary.keys():\n",
    "        if len(route_dictionary[k]) == 1: # this step eliminate dummy trucks like #0,#1 trucks doing nothing\n",
    "            continue\n",
    "        for v in route_dictionary[k]:\n",
    "            df = pd.concat([df, pd.DataFrame({'truck_number':[str(k)+b],'pick_node':[v]})])\n",
    "    return df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_index(df,x):\n",
    "    '''\n",
    "    param:\n",
    "        df: distance matrix with UNIQUE index & columns\n",
    "        x: truck location source and truck location next-stop \n",
    "    return:\n",
    "        DataFrame: distance matrix\n",
    "    '''\n",
    "    try:\n",
    "        return df.loc[x[0],x[1]]\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for b in range((287 // 100)+1):\n",
    "    print(f'{b*100} - {(b+1)*100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\U279014\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: RuntimeWarning: invalid value encountered in arccos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35 57 24 57 62]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\U279014\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: RuntimeWarning: invalid value encountered in arccos\n"
     ]
    }
   ],
   "source": [
    "route_by_freq = pd.DataFrame()\n",
    "for f in freqs:\n",
    "    \n",
    "#     if f == 'weekly':\n",
    "#         continue\n",
    "    \n",
    "#     if f == 'bi_weekly':\n",
    "#         continue\n",
    "\n",
    "#     if f == 'monthly':\n",
    "#         break \n",
    "    \n",
    "    ranks = sorted(cluster_copy[cluster_copy.freq == f]['label'].unique())\n",
    "\n",
    "    route_by_rank = pd.DataFrame()\n",
    "    for i, r in enumerate(ranks):\n",
    "        \n",
    "        label_no = Counter(cluster_copy[cluster_copy.freq == f]['label']).most_common()[r][0]\n",
    "        cluster = cluster_copy[(cluster_copy.label == label_no) & (cluster_copy.freq == f)]\n",
    "        cluster = cluster.sort_values(by = 'ship_weight', ascending=False)\n",
    "        \n",
    "        greenville = pd.DataFrame([['54942', 'GREENVILLE', f, 0, 0, 0, 0, '54942', -88.53557,44.293820, 'WI',label_no,'10-01-2019']], columns=cluster.columns)\n",
    "        chambersburg = pd.DataFrame([['17201', 'CHAMBERSBURG_WH', f, 0, 0, 0, 0, '17201', -77.6614, 39.93112,'PA',label_no,'01-01-2019']], columns=cluster.columns)\n",
    "        \n",
    "        cass_zip_cluster_copy = greenville.append(cluster).reset_index(drop = True)\n",
    "        vrp_size = cass_zip_cluster_copy.shape[0]\n",
    "        if vrp_size>100:\n",
    "            k = (vrp_size // 100) + 1\n",
    "            route_over_100 = pd.DataFrame()\n",
    "            \n",
    "            element_counts_max = 101\n",
    "            while element_counts_max >= 100:\n",
    "\n",
    "                kmeans = KMeans(k, random_state=0).fit(cass_zip_cluster_copy.loc[:, ['longitude', 'latitude']])\n",
    "                id_labels=kmeans.labels_\n",
    "                elements, element_counts = np.unique(id_labels, return_counts = True)\n",
    "                element_counts_max = element_counts.max()\n",
    "                cass_zip_cluster_copy['k_label'] = id_labels\n",
    "                k+=1\n",
    "            \n",
    "            print(element_counts)\n",
    "      \n",
    "            for b in set(id_labels):\n",
    "                cass_zip_toy = cass_zip_cluster_copy[(cass_zip_cluster_copy.k_label == b) & (cass_zip_cluster_copy.shipper_name != 'GREENVILLE')]\n",
    "\n",
    "                cass_zip_toy = greenville.append(cass_zip_toy).reset_index(drop = True)\n",
    "                \n",
    "                distance_matrix_toy = distance_on_sphere_numpy(cass_zip_toy)\n",
    "        \n",
    "                unique_cass_zip_toy = cass_zip_toy.drop_duplicates(subset=['zip_code'])\n",
    "                unique_distance_matrix_toy = distance_on_sphere_numpy(unique_cass_zip_toy)   \n",
    "                df_unique_distance_matrix = pd.DataFrame(unique_distance_matrix_toy, index=unique_cass_zip_toy.zip_code, columns=unique_cass_zip_toy.zip_code)\n",
    "                ship_weight_list_toy = cass_zip_toy.ship_weight.tolist()\n",
    "\n",
    "                route_dictionary = vrp_master(distance_matrix=distance_matrix_toy)\n",
    "                route_schedule_result = route_schedule(route_dictionary, b = str(b))\n",
    "\n",
    "                route_in_weight = route_schedule_result.merge(cass_zip_toy, left_on='pick_node',right_index=True,how='left')\n",
    "                route_in_weight['next_zip_code'] = route_in_weight.groupby(['truck_number'])['zip_code'].shift(-1)\n",
    "                route_in_weight['next_shipper_name'] = route_in_weight.groupby(['truck_number'])['shipper_name'].shift(-1)\n",
    "                route_in_weight['milk_run_distance'] = route_in_weight[['zip_code','next_zip_code']].apply(lambda x: round(distance_index(df_unique_distance_matrix,x)),axis=1)\n",
    "                route_in_weight['stop_number'] = route_in_weight.groupby('truck_number').cumcount()\n",
    "\n",
    "                route_over_100 = pd.concat([route_over_100, route_in_weight])\n",
    "                            \n",
    "            route_by_rank = pd.concat([route_by_rank, route_over_100])\n",
    "\n",
    "        else:\n",
    "            cass_zip_toy = cass_zip_cluster_copy.copy().reset_index(drop = True)\n",
    "\n",
    "        \n",
    "            distance_matrix_toy = distance_on_sphere_numpy(cass_zip_toy)\n",
    "\n",
    "            unique_cass_zip_toy = cass_zip_toy.drop_duplicates(subset=['zip_code'])\n",
    "            unique_distance_matrix_toy = distance_on_sphere_numpy(unique_cass_zip_toy)   \n",
    "            df_unique_distance_matrix = pd.DataFrame(unique_distance_matrix_toy, index=unique_cass_zip_toy.zip_code, columns=unique_cass_zip_toy.zip_code)\n",
    "            ship_weight_list_toy = cass_zip_toy.ship_weight.tolist()\n",
    "\n",
    "            route_dictionary = vrp_master(distance_matrix=distance_matrix_toy)\n",
    "\n",
    "            \n",
    "            route_schedule_result = route_schedule(route_dictionary)\n",
    "            \n",
    "\n",
    "            route_in_weight = route_schedule_result.merge(cass_zip_toy,left_on='pick_node',right_index=True,how='left')\n",
    "            route_in_weight['next_zip_code'] = route_in_weight.groupby(['truck_number'])['zip_code'].shift(-1)\n",
    "            route_in_weight['next_shipper_name'] = route_in_weight.groupby(['truck_number'])['shipper_name'].shift(-1)\n",
    "            route_in_weight['milk_run_distance'] = route_in_weight[['zip_code','next_zip_code']].apply(lambda x: round(distance_index(df_unique_distance_matrix,x)),axis=1)\n",
    "            route_in_weight['stop_number'] = route_in_weight.groupby('truck_number').cumcount()\n",
    "\n",
    "            route_by_rank = pd.concat([route_by_rank, route_in_weight])\n",
    "            \n",
    "\n",
    "    route_by_freq = pd.concat([route_by_freq, route_by_rank])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following is origin"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "route_by_freq = pd.DataFrame()\n",
    "for f in freqs:\n",
    "    \n",
    "#     if f == 'weekly':\n",
    "#         continue\n",
    "    \n",
    "#     if f == 'bi_weekly':\n",
    "#         continue\n",
    "\n",
    "#     if f == 'monthly':\n",
    "#         break \n",
    "    \n",
    "    ranks = sorted(cluster_copy[cluster_copy.freq == f]['label'].unique())\n",
    "\n",
    "    route_by_rank = pd.DataFrame()\n",
    "    for i, r in enumerate(ranks):\n",
    "        \n",
    "        label_no = Counter(cluster_copy[cluster_copy.freq == f]['label']).most_common()[r][0]\n",
    "        cluster = cluster_copy[(cluster_copy.label == label_no) & (cluster_copy.freq == f)]\n",
    "        cluster = cluster.sort_values(by = 'ship_weight', ascending=False)\n",
    "        \n",
    "        greenville = pd.DataFrame([['54942', 'GREENVILLE', f, 0, 0, 0, 0, '54942', -88.53557,44.293820, 'WI',label_no,'10-01-2019']], columns=cluster.columns)\n",
    "        chanbersburg = pd.DataFrame([['17201', 'CHANBERSBURG_WH', f, 0, 0, 0, 0, '17201', -77.6614, 39.93112,'PA',label_no,'01-01-2019']], columns=cluster.columns)\n",
    "        \n",
    "        cass_zip_cluster_copy = greenville.append(cluster).reset_index(drop = True)\n",
    "        vrp_size = cass_zip_cluster_copy.shape[0]\n",
    "        if vrp_size>100:\n",
    "            print(f'input vrp_size:{vrp_size} exceeds 100 limits')\n",
    "            batch = vrp_size // 100\n",
    "            route_over_100 = pd.DataFrame()\n",
    "            for b in range(batch+1):\n",
    "                cass_zip_toy = cass_zip_cluster_copy[b*100:(b+1)*100].reset_index(drop = True)\n",
    "                \n",
    "                distance_matrix_toy = distance_on_sphere_numpy(cass_zip_toy)\n",
    "        \n",
    "                unique_cass_zip_toy = cass_zip_toy.drop_duplicates(subset=['zip_code'])\n",
    "                unique_distance_matrix_toy = distance_on_sphere_numpy(unique_cass_zip_toy)   \n",
    "                df_unique_distance_matrix = pd.DataFrame(unique_distance_matrix_toy, index=unique_cass_zip_toy.zip_code, columns=unique_cass_zip_toy.zip_code)\n",
    "                ship_weight_list_toy = cass_zip_toy.ship_weight.tolist()\n",
    "\n",
    "                route_dictionary = vrp_master()\n",
    "                route_schedule_result = route_schedule(route_dictionary)\n",
    "\n",
    "                route_in_weight = route_schedule_result.merge(cass_zip_toy, left_on='pick_node',right_index=True,how='left')\n",
    "                route_in_weight['next_zip_code'] = route_in_weight.groupby(['truck_number'])['zip_code'].shift(-1)\n",
    "                route_in_weight['next_shipper_name'] = route_in_weight.groupby(['truck_number'])['shipper_name'].shift(-1)\n",
    "                route_in_weight['milk_run_distance'] = route_in_weight[['zip_code','next_zip_code']].apply(lambda x: round(distance_index(df_unique_distance_matrix,x)),axis=1)\n",
    "                route_in_weight['stop_number'] = route_in_weight.groupby('truck_number').cumcount()\n",
    "\n",
    "                route_over_100 = pd.concat([route_over_100, route_in_weight])\n",
    "                            \n",
    "            route_by_rank = pd.concat([route_by_rank, route_over_100])\n",
    "\n",
    "        else:\n",
    "            cass_zip_toy = cass_zip_cluster_copy.copy().reset_index(drop = True)\n",
    "#             print(cass_zip_toy.shipper_name)\n",
    "        \n",
    "            distance_matrix_toy = distance_on_sphere_numpy(cass_zip_toy)\n",
    "\n",
    "            unique_cass_zip_toy = cass_zip_toy.drop_duplicates(subset=['zip_code'])\n",
    "            unique_distance_matrix_toy = distance_on_sphere_numpy(unique_cass_zip_toy)   \n",
    "            df_unique_distance_matrix = pd.DataFrame(unique_distance_matrix_toy, index=unique_cass_zip_toy.zip_code, columns=unique_cass_zip_toy.zip_code)\n",
    "            ship_weight_list_toy = cass_zip_toy.ship_weight.tolist()\n",
    "\n",
    "            route_dictionary = vrp_master()\n",
    "#             print(f'{route_dictionary}\\n')\n",
    "            \n",
    "            route_schedule_result = route_schedule(route_dictionary)\n",
    "            \n",
    "\n",
    "            route_in_weight = route_schedule_result.merge(cass_zip_toy,left_on='pick_node',right_index=True,how='left')\n",
    "            route_in_weight['next_zip_code'] = route_in_weight.groupby(['truck_number'])['zip_code'].shift(-1)\n",
    "            route_in_weight['next_shipper_name'] = route_in_weight.groupby(['truck_number'])['shipper_name'].shift(-1)\n",
    "            route_in_weight['milk_run_distance'] = route_in_weight[['zip_code','next_zip_code']].apply(lambda x: round(distance_index(df_unique_distance_matrix,x)),axis=1)\n",
    "            route_in_weight['stop_number'] = route_in_weight.groupby('truck_number').cumcount()\n",
    "\n",
    "            route_by_rank = pd.concat([route_by_rank, route_in_weight])\n",
    "            \n",
    "\n",
    "    route_by_freq = pd.concat([route_by_freq, route_by_rank])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "route_by_freq.to_csv(r'C:\\Users\\u279014\\Documents\\H_Drive\\7.AA Models\\12.Logistic_Optimization\\data\\route_test_2.csv',index=True,index_label='time_sequence')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# delete later .... only for test purpose\n",
    "route_in_weight.to_csv(r'C:\\Users\\u279014\\Documents\\H_Drive\\7.AA Models\\12.Logistic_Optimization\\data\\route_test.csv',index=True,index_label='time_sequence')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "route_in_weight.to_csv(r'C:\\Users\\u279014\\Documents\\H_Drive\\7.AA Models\\12.Logistic_Optimization\\data\\route_in_weight.csv',index=True,index_label='time_sequence')\n",
    "# route_in_weight.to_csv(r'S:\\CORP-Share\\DEPT\\IT\\DT-AA\\FY20\\GPSC\\UseCases\\8. Logistics Route Optimization\\route_in_weight.csv',index=True,index_label='time_sequence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Analytical Result: Miles & Cost Saving Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>zip_code</th>\n",
       "      <th>54942</th>\n",
       "      <th>98138</th>\n",
       "      <th>98188</th>\n",
       "      <th>98108</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zip_code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54942</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1602.825956</td>\n",
       "      <td>1624.673752</td>\n",
       "      <td>1626.005038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98138</th>\n",
       "      <td>1602.825956</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>22.061098</td>\n",
       "      <td>25.156266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98188</th>\n",
       "      <td>1624.673752</td>\n",
       "      <td>22.061098</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>6.887259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98108</th>\n",
       "      <td>1626.005038</td>\n",
       "      <td>25.156266</td>\n",
       "      <td>6.887259</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "zip_code        54942        98138        98188        98108\n",
       "zip_code                                                    \n",
       "54942        0.000000  1602.825956  1624.673752  1626.005038\n",
       "98138     1602.825956     0.000059    22.061098    25.156266\n",
       "98188     1624.673752    22.061098     0.000059     6.887259\n",
       "98108     1626.005038    25.156266     6.887259     0.000000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distance matrix\n",
    "df_unique_distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tmc_miles = route_in_weight.miles.sum()\n",
    "total_milk_miles = route_in_weight.milk_run_distance.sum()\n",
    "miles_saving = (total_tmc_miles-total_milk_miles)\n",
    "print('-original_miles:{0} \\n-milkrun_miles:{1}\\n-miles reducton:{2}'.format(total_tmc_miles,total_milk_miles,miles_saving))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <<<<<<  Modeling Completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Financial Impact >>>>>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path,file,sheet_name = None):\n",
    "    df = pd.read_excel(os.path.join(path,file),sheet_name=sheet_names)\n",
    "    df = pd.concat(df[frame] for frame in df.keys())\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df.to_feather(os.path.join(path,'tmc_feather'))\n",
    "    return feather.read_dataframe(os.path.join(path,'tmc_feather'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\u279014\\Documents\\H_Drive\\7.AA Models\\12.Logistic_Optimization\\data'\n",
    "file = r'TMC_freight_rate.xlsx'\n",
    "sheet_names = ['Phase 1','Phase 2','Phase 3','Phase 4','Phase 5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data(path=path,file=file,sheet_name=sheet_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize dataframe colume names\n",
    "def col_name(df):\n",
    "    \"\"\"\n",
    "    this is to trim the data_frame column names to a unique format:\n",
    "    all case, replace space to underscore, remove parentheses\n",
    "    param df:\n",
    "        raw from share drive for\n",
    "    return:\n",
    "        polished data set with new column names\n",
    "    \"\"\"\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace('-','').str.replace(' ', '_').str.replace('(', '').\\\n",
    "                    str.replace(')', '').str.replace('\"','')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Slice tmc \"\"\"\n",
    "def clean_tmc(df, sink_state = 'WI', source_states = 'IL'):\n",
    "    \"\"\"\n",
    "    parameter: \n",
    "        df: original TMC dataset\n",
    "        sink_state: destination warehouse, only one locations allowed\n",
    "        source_states: shipping states, allowing multiple states as source state\n",
    "    return:\n",
    "        cleaned TMC including freight_cost from all states to sink_state\n",
    "    \"\"\"\n",
    "    # starndardize col name\n",
    "    df = col_name(df)\n",
    "    \n",
    "    # drop rows if all cols are nan\n",
    "    df.dropna(how='all',subset=['market_rate_over_quarter_decmar',\n",
    "       'market_rate_over_jan_2019mar_2020',\n",
    "       'market_rate_all_offers_jan_2019_mar_2020_no_fb',\n",
    "       'market_rate_all_offers_jan_2019_mar_2020_with_fb'],inplace=True)\n",
    "    \n",
    "    # generate freight_cost = market_rate_all_offers_jan_2019_mar_2020_no_fb or max of all\n",
    "    df['freight_cost'] = np.round(np.where(df.market_rate_all_offers_jan_2019_mar_2020_no_fb.isnull(),\n",
    "                               np.max(df,axis=1),\n",
    "                               df.market_rate_all_offers_jan_2019_mar_2020_no_fb),2)  \n",
    "    df['source_state'] = df.lane.apply(lambda x: x[:2]) # find source state short code\n",
    "    df['sink_state'] = df.lane.apply(lambda x: x[-2:]) # find sink state short code\n",
    "    \n",
    "    df = df[df.source_state.isin(source_states)] # slice only source state\n",
    "    df = df[df.sink_state.str.contains(sink_state)] # slice to include destination state only\n",
    "    df = df.groupby(['source_state','sink_state'])['freight_cost'].mean().reset_index() # average duplidate states to same destination, \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate cleaned TMC dataset\n",
    "source_states = cluster.shipper_state.unique()\n",
    "tmc = clean_tmc(df, sink_state='WI', source_states = source_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating full truck load cost\n",
    "route_in_weight['milk_run_cost'] = 0\n",
    "TL_cost = np.max(tmc.freight_cost)\n",
    "route_in_weight.loc[route_in_weight.groupby('truck_number').tail(1).index,'milk_run_cost'] = TL_cost\n",
    "route_in_weight.to_csv(r'C:\\Users\\u279014\\Documents\\H_Drive\\7.AA Models\\12.Logistic_Optimization\\data\\route_in_weight.csv',index=True,index_label='time_sequence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truck_used = len(route_in_weight.truck_number.unique())\n",
    "total_tmc_billed = route_in_weight.billed_amount.sum()\n",
    "total_milk_cost = round(np.max(tmc.freight_cost)*truck_used,2)\n",
    "# total_milk_cost = round(float(tmc.freight_cost)*truck_used,2)\n",
    "cost_saving = round((total_tmc_billed - total_milk_cost),2)\n",
    "print('-original_cost:{0} \\n-milkrun_cost:{1}\\n-cost reducton:{2}'.format(total_tmc_billed,total_milk_cost,cost_saving))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add potential Oshkosh Hubs to the route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clustering_main as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hub_dict(path, file, destination_list, route_in_weight, inbound_indicator='INBOUND'):\n",
    "    \"\"\"\n",
    "    param:\n",
    "        file: Cass FY19 Invoice Detail.csv\n",
    "        inbound_indicator: str\n",
    "        destination_list: list\n",
    "    return:\n",
    "        osk_hub_dict: dictionary, {supplier_name: [osk_warehouses...]\n",
    "    \"\"\"\n",
    "    _data = cm.ETL_data(path=path).col_name(file=file)\n",
    "    _data = _data[_data.inbound_outbound_indicator == inbound_indicator]\n",
    "    df_hub_dict = _data[_data.destination_city.isin(destination_list)][['shipper_name', 'shipper_city', 'shipper_state', 'shipper_zip', 'destination_city', 'destination_state', 'destination_zip']]\n",
    "    df_hub_dict = df_hub_dict.drop_duplicates(subset=['shipper_name', 'destination_city'])\n",
    "    df_hub_dict = df_hub_dict[df_hub_dict.shipper_name.isin(set(route_in_weight.shipper_name))]\n",
    "    df_hub_dict = df_hub_dict[df_hub_dict.shipper_zip.isin(set(route_in_weight.zip_code))]\n",
    "\n",
    "    hub_dict = defaultdict(set)\n",
    "    for sn, dc in zip(_data.shipper_name, _data.destination_city):\n",
    "        if dc in destination_list:\n",
    "            hub_dict[sn].add(dc)\n",
    "        else:\n",
    "            pass\n",
    "    return hub_dict, df_hub_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\u279014\\\\Documents\\\\H_Drive\\\\7.AA Models\\\\12.Logistic_Optimization\\\\data'\n",
    "file = 'Cass FY19 Invoice Detail.csv'\n",
    "destination_list = ['MILWAUKEE', 'OSHKOSH', 'GREENVILLE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_dictionary, df_hub_dictionary = hub_dict(path=path, file=file, destination_list=destination_list, route_in_weight=route_in_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_hub_dictionary.to_csv('hub_dictionary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "route_in_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hub_dictionary[df_hub_dictionary.shipper_name.isin(set(route_in_weight.shipper_name))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
